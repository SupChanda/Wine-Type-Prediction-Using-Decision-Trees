{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8 Report\n",
    "\n",
    "## Team members: Manish Meshram, Supratik Chanda\n",
    "\n",
    "## Introduction\n",
    "The idea behind our analysis is to use machine learning techniques and algorithms to determine which physiochemical properties make a wine 'good'! The dataset is related to red variants of the Portuguese \"Vinho Verde\" wine. We have used decision trees and neural networks to train the model on the data and predict the quality of wine. Both the models performed similar in the experiments.\n",
    "\n",
    "## Dataset\n",
    "The dataset contains 1599 instances of physiochemical properties of wine. All the wine samples are related to red variants of Portuguese \"Vinho Verde\" wine. This dataset was pretty clean with no null values. Following are the independent variables we have in our dataset:\n",
    "1. fixed acidity \n",
    "2. volatile acidity \n",
    "3. citric acid \n",
    "4. residual sugar \n",
    "5. chlorides \n",
    "6. free sulfur dioxide \n",
    "7. total sulfur dioxide \n",
    "8. density \n",
    "9. pH \n",
    "10. sulphates \n",
    "11. alcohol\n",
    "\n",
    "The dependent variable is 'quality' which is the score between 1 and 10 where 1 represents poor quality and 10 represents the highest quality of wine. The dataset was unbalanced between the different categories of 'quality', to make it a balanced dataset and to also deliver more value from our analysis, we have divided the data amongst two categories; 'good' and 'bad' and trained our models on the transformed data.\n",
    "\n",
    "\n",
    "## Analysis technique\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "To start with the analysis we first studied the data and checked if it is suitable for modelling and further analysis. We started with plotting out the frequencies of our dependent categories just to get an idea about the distribution of our dependent variable. Once we figured out that it is quite unbalanced, we modified the target variable and divided the data into two classes based on the value of 'quality' of each wine. We have used a criteria of naming a wine 'bad' if its quality is <=5 and 'good' if its quality >=6. The other reason for dividing the data is because it seemed to be delivering more value to the user where we just tell him whether the quality of wine is 'good' or 'bad' rather than giving a quality point to the wine.\n",
    "\n",
    "We have also performed correlations between the variables and plotted a heatmap to get more idea about correlations amongst all the variables.\n",
    "\n",
    "####-----write about feature selections---\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "####-----write about decision tree analysis---\n",
    "\n",
    "#### Neural Networks\n",
    "Neural networks seemed to be a good bet after decision trees. We have used sklearn's Multi Layer Perceptron library for our analysis. We have followed a train-test split of 75-25 and also scaled the data based on mean and standard deviation before feeding it to the training. We have trained 3 different neural networks where we tried a bunch of different parameters while training and selected the best ones based on the performance. During this exercise we tried various hidden layers with different number of neurons, 'logistic' & 'relu' activation functions and 'adam' & 'sgd' as our solvers. We have obeserved that 'relu' activation function with 'adam' solver gives best results if all the other parameters kept constant. We have also tried different values of maximum iterations while training the models ranging from 500 to 10000.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "The following is the frequency plot of each 'quality' category in the data:\n",
    "\n",
    "![frequency](EDA/categories_frequency.png)\n",
    "\n",
    "From the plot it can be seen that the data is highly unbalanced across the different categories and also we don't have very poor or very high quality wine in our dataset. To make it more balanced we divided the data into two categories namely 'good' and 'bad' based on the criteria expalined in the Analysis section. After breaking it down we got a quite balanced data with 744 instances in Bad quality(class=0) and 855 instances in Good quality(class=1) for wine.\n",
    "\n",
    "We also checked the correlations amongst different variables in the data and here are the results:\n",
    "\n",
    "![fcorr](EDA/correlation.png)\n",
    "\n",
    "Correlation heatmap shows that there ......(should I include new target variable)\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "####-----write about decision tree results---\n",
    "\n",
    "\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "We have tried various neural network architectures and trained the models by changing the number of hidden layers, neurons in hidden layers, different activation functions and solvers and finalized the following three architectures that performed well:\n",
    "\n",
    "##### Neural Network 1\n",
    "Architecture: 11 x 2 x 2 x 2 (2 hidden layers with 2 neurons in each layer)<br>\n",
    "Solver: Adam<br>\n",
    "Activation function: ReLU<br>\n",
    "Maximum iterations: 500\n",
    "\n",
    "![nn1](NeuralNetworks/NN1.png)\n",
    "\n",
    "*Results:*\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.67      0.73      0.70       178\n",
    "        1.0       0.77      0.71      0.74       222\n",
    "    avg / total   0.72      0.72      0.72       400\n",
    "\n",
    "\n",
    "##### Neural Network 2\n",
    "Architecture: 11 x 15 x 15 x 15 x 2 (3 hidden layers with 15 neurons in each layer)<br>\n",
    "Solver: Adam<br>\n",
    "Activation function: ReLU<br>\n",
    "Maximum iterations: 10000\n",
    "\n",
    "![nn2](NeuralNetworks/NN2.png)\n",
    "\n",
    "*Results:*\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.75      0.75      0.75       178\n",
    "        1.0       0.80      0.80      0.80       222\n",
    "    avg / total   0.78      0.78      0.78       400\n",
    "\n",
    "\n",
    "##### Neural Network 3\n",
    "Architecture: 11 x 30 x 30 x 15 x 2 (3 hidden layers with 30, 30 and 15 neurons in each layer respectively)<br>\n",
    "Solver: Adam<br>\n",
    "Activation function: ReLU<br>\n",
    "Maximum iterations: 10000\n",
    "\n",
    "![nn3](NeuralNetworks/NN3.png)\n",
    "\n",
    "*Results:*\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.78      0.79      0.79       178\n",
    "        1.0       0.83      0.82      0.83       222\n",
    "    avg / total   0.81      0.81      0.81       400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments relating to code snippet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code snippet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments relating to code snippet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
