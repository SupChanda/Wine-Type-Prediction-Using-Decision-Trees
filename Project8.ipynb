{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8 Report\n",
    "\n",
    "## Team members: Manish Meshram, Supratik Chanda\n",
    "\n",
    "## Introduction\n",
    "The idea behind our analysis is to use machine learning techniques and algorithms to determine which physiochemical properties make a wine 'good'! The dataset is related to red variants of the Portuguese \"Vinho Verde\" wine. We have used decision trees and neural networks to train the model on the data and predict the quality of wine. Both the models performed similar in the experiments.For decision tree, we have also analyzed the effect of gini feature importance and how they are correlated with finding node impurity.\n",
    "\n",
    "## Dataset\n",
    "The dataset contains 1599 instances of physiochemical properties of wine. All the wine samples are related to red variants of Portuguese \"Vinho Verde\" wine. This dataset was pretty clean with no null values. Following are the independent variables we have in our dataset:\n",
    "1. fixed acidity \n",
    "2. volatile acidity \n",
    "3. citric acid \n",
    "4. residual sugar \n",
    "5. chlorides \n",
    "6. free sulfur dioxide \n",
    "7. total sulfur dioxide \n",
    "8. density \n",
    "9. pH \n",
    "10. sulphates \n",
    "11. alcohol\n",
    "\n",
    "The dependent variable is 'quality' which is the score between 1 and 10 where 1 represents poor quality and 10 represents the highest quality of wine. The dataset was unbalanced between the different categories of 'quality', to make it a balanced dataset and to also deliver more value from our analysis, we have divided the data amongst two categories; 'good' and 'bad' and trained our models on the transformed data.\n",
    "Data Cleansing Techniques:\n",
    "Check any column or axis has a null value or not.If there is a null value, remove that column to make accurate prediction\n",
    "Check the datatypes of each column so that each datatype is of float or int\n",
    "Remove the duplicate rows that are present in the dataset \n",
    "Reseting the index of the unique rows of the dataframe\n",
    "\n",
    "\n",
    "## Analysis technique\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "To start with the analysis we first studied the data and checked if it is suitable for modelling and further analysis. We started with plotting out the frequencies of our dependent categories just to get an idea about the distribution of our dependent variable. Once we figured out that it is quite unbalanced, we modified the target variable and divided the data into two classes based on the value of 'quality' of each wine. We have used a criteria of naming a wine 'bad' if its quality is <=5 and 'good' if its quality >=6. The other reason for dividing the data is because it seemed to be delivering more value to the user where we just tell him whether the quality of wine is 'good' or 'bad' rather than giving a quality point to the wine.\n",
    "\n",
    "We have also performed correlations between the variables and plotted a heatmap to get more idea about correlations amongst all the variables.\n",
    "\n",
    "We also found out the most important features from the independent variables.\n",
    "For this, we used the SelectKBest function of the feature Selection package of Scikit learn.\n",
    "We used chi2 as the score function and found out the p-values and scores  of every column of X Feature.\n",
    "Then, we created a dataframe containing the name of the X Features columns, their p_values and the scores from highest p_values and score to the lowest.\n",
    "From the 10 features, according to the p_values we took the first 6 columns with highest p_values for the decisiontree classifier\n",
    "Surprisingly when we counted the number of unique values of the Y Label, there is a high percentage of few Labels and extremely low number of other Labels like 1 and 2. \n",
    "For experimental purposes, we used the top 5 features obtained from SelectKBest feature selection technique, that have most significant p-values. Even though on using the features, the performance didn't increase . The F1 score was identical to the F1 score that we found out using all features. Hence, we used all the X Features. The below experiments use all X features.\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "To kick off the analysis, we used the feature importance or gini importance of every column using the feature_importances_ function of decisiontreeclassifier.\n",
    "\n",
    "We used 3 types of tuning parameters of decisiontreeclassifier:\n",
    "1) Minimum samples of leaf at each node(internal and leaf)\n",
    "2) Maximum depth of the tree\n",
    "3) Maximum leaf nodes\n",
    "\n",
    "#### 1) Minimum samples of leaf at each node(internal and leaf)\n",
    "\n",
    "###### Part1 Analysis: Max_depth is 3 and max_leaf_nodes is 10 while min_samples_leaf is 1000\n",
    "\n",
    "<img src='DecisionTree/min_samples_pic_1.png' style='height:200px'/>\n",
    "\n",
    "This image provides a detailed analysis why although the maximum depth is 3, but there are no internal nodes as well as leaf nodes. \n",
    "There is only one node. This is because of the parameter known as minimum samples of leaf at each node. \n",
    "As we see that, the min_samples_leaf is equal to 1000. So, if the leaf has to be splitted into two other leaf nodes, then the samples would have been less than 1000.\n",
    "Henceforth, although the max_depth is 3 , the leaf has no branch.\n",
    "The bias-variance tradeoff does depend on the depth of the tree. Since, the depth of the tree is 0 , the tree has high bias and low variance. That means, that the tree wil make huge assumptions in predicting the target Label while greatly change if we change few samples in training dataset.\n",
    "If we look at the gini parameter, it is 0.648. This means that the node is not at its purest level.\n",
    "As you can see, the root node (depth=0) didn't chose any X Feature as the splitting feature.\n",
    "\n",
    "######  Part2 Analysis: Max_depth is 3 and max_leaf_nodes is 10 while min_samples_leaf is 500\n",
    "\n",
    "<img src='DecisionTree/min_samples_pic_2.png' style='height:200px'/>\n",
    "\n",
    "As you can see in the figure, the maximum depth is 1 not 3.\n",
    "This is because of the minimum_samples_split.If the nodes were splitted further into leaf nodes, then the minimum no of samples of each leaf node would have been lesser than 500 and that contradicts the parameter min_samples_leaf which is 500.\n",
    "Therefore each sample at every node has a value greater than 500.\n",
    "Hence, proved that min_samples_leaf has a greater effect than max_depth.\n",
    "The root node (depth=0) chose \"alcohol\" as the splitting feature, and 10.15 as the threshold value. \n",
    "The bias-variance tradeoff does depend on the depth of the tree. Since, the depth of the tree is 1 , the tree has relatively less higher bias and a bit higher variance. That means, that the tree will make less huge assumptions in predicting the target Label while change to a certain amount if we change few samples in training dataset.\n",
    "The gini impurity is 0.648 and still high. It is still far away than its purest form .\n",
    "The two depth 1 nodes didn't chose any of the X Features as the their splitting feature, respectively.\n",
    "The two leaf nodes (depth=1) each predicts a class - one of them predict class_5 and one predicts class_6. \n",
    "Both of the leaf nodes have a large gini impurity (0.517 and 0.656). Hence, the tree suggests that it is not fitted properly and can be further broken down.\n",
    "\n",
    "###### Part3 Analysis: Max_depth is 3 and max_leaf_nodes is 10 while min_samples_leaf is 100\n",
    "\n",
    "<img src='DecisionTree/min_samples_split_1.png' style='height:300px'/>\n",
    "\n",
    "As it is seen, in this case the max_depth is 3 and not less.\n",
    "This is because even after reaching a depth of 3 , the leaf nodes' samples count is more than 100. The least sample leaf count is 103.\n",
    "The total leaf nodes count is exactly 10. \n",
    "If the max leaf nodes were 2  then although the min_samples_leaf would be more than 500, max_depth would be 1\n",
    "This is because the max_leaf_nodes were attained and no more spitting would happen. \n",
    "The highest and most significant gini value is found to be 0.244 which determines a class of 5. For this particular node, the leaf has low bias and high variance. For the other cases, since the node is not pure, the node has high bias and low variance.\n",
    "\n",
    "#### 2) Maximum depth of the tree\n",
    "###### Part 1 Analysis: Max_depth is 1  while min_samples_leaf is 10\n",
    "<img src='DecisionTree/max_depth_1.png' style='height:400px'/>\n",
    "\n",
    "This image provides a detailed analysis why depth is 1 although the minimum samples leaf is 10.\n",
    "As you can see, the root node (depth=0) chose \"alcohol\" as the splitting feature, and 10.15 as the threshold value. The two depth 1 nodes has gini value as 0.517 and 0.656 which means that the leaf node is not all homogenous and has a high bias. These attributes are explained below:\n",
    "In each node, \"samples\" represents how many training instances fall in that node. For example, there are 1359 samples that has alcohol <= 10.15.\n",
    "The attribute \"value\" gives how many instances fall in each class for each node. For example, in the leftmost leaf node, there are 676 instances in class_5,  and 683 instances in class_6.\n",
    "Each node is also assigned a \"class\" based on which class has the majority of the instances in that node. Therefore, even though the root node has comparable number of instances in each of the three classes, the root node's class label is \"class_5\" since 425 > 197 > 29.....\n",
    "To make a prediction, you keep going along the tree until you reach a leaf node. The probability of each class is the fraction of training samples of each class in a leaf node. For example, the bottom left leaf node predicts class_2 with a probaility of  425/676≈ 62.0%. This attribute can be obtained through DecisionTreeClassifier's predict_proba() function. Note that all samples that fall in the same leaf node share the same prediction probability.\n",
    "\n",
    "###### Now if we add max_leaf_nodes in the analysis:Max_depth is 1 ,max_leaf_nodes is 3 while min_samples_leaf is 10.\n",
    "<img src='DecisionTree/max_depth_2.png' style='height:400px'/>\n",
    "\n",
    "The first question that comes to mind is although the max depth is 1 , then why in the diagram the max depth is 2. This is because of the parameter max_leaf_nodes. \n",
    "Although the max depth is 1, the algorthim prints the tree till the max_leaf_nodes is not exhausted. In depth 1, the leaf nodes are 2 which is less than 3 . Hence the depth goes down to level 2 and then the max_leaf_nodes become 3. From this we can infer that max_leaf_nodes has a greater priority over max_depth.\n",
    "The gini impurity is still high and far away from giving homogenous nodes.\n",
    "In each node, \"samples\" represents how many training instances fall in that node. For example, there are 1359 samples that has alcohol <= 10.15.\n",
    "So, the parent node is divided into two internal nodes,one with alcohol > 10.15 and other with alcohol <=10.15 and total sulfur dioxide <=98.5 .The internal node with total sulfur dioxide is further divided into two leaf nodes.\n",
    "For Leaf Node 1: gini is 0.552, that implies that the leaf node has a high bias . Node 2 has a good gini though, 0.106 close to 0 and that tells that the leaf nodes is rather pure with a low bias. \n",
    "For the leaf node which has alcohol value > 10.15, we have a bad gini which is equal to 0.656.\n",
    "\n",
    "#### Part 2 Analysis: Max_depth is 5 and max_leaf_nodes is 10 while min_samples_leaf is 10\n",
    "\n",
    "In this case,the algorithm first goes to a depth of 5 and sees that leaf nodes produced by the tree is 10. So all the criteria is matched and hence no more depth.\n",
    "The best gini parameter that we got in this tree is of 0.106. The gini impurity is close to 0.0. Hence,for this node we can say that it is highly homogenous and have a class_5\n",
    "\n",
    "###### First branch(left side) details where all  the nodes are highlighted:\n",
    "\n",
    "<img src='DecisionTree/max_depth_3.png' style='height:400px'/>\n",
    "\n",
    "First internal node(total sulfur dioxide) is formed by one condition: alcohol < = 10.15 \n",
    "The next two internal nodes are formed by: total sulfur dioxide .One leaf node with total sulfur dioxide > 98.5 with a great gini value 0.106.The other internal node with total sulfur dioxide <=98.5 and lower gini value 0.552\n",
    "The next two internal nodes are formed by: sulphates.One leaf node with sulphates<=0.585.The other node with condition sulphates>0.585.\n",
    "The node with sulphates >0.585 is further divided into two nodes. The condition is volatile acidity <=0.548\n",
    "The nodes formed by the condition volatile acidity <=0.548 are the leaf nodes.\n",
    "\n",
    "###### Second branch(right side) details where all the nodes are highlighted:\n",
    "\n",
    "<img src='DecisionTree/max_depth_4.png' style='height:400px'/>\n",
    "\n",
    "irst internal node(alcohol <=11.55) is formed by one condition: alcohol > 10.15 \n",
    "The internal node alcohol <=11.55 is broken into two nodes.One node with volatile acidity <=0.375 with a  gini value 0.638.The other internal node with sulphates <=0.685 and has a  gini value 0.621\n",
    "The node containing volatile acidity <=0.375 is further divided into two nodes.One is a leaf node and the other is an internal node. The internal node has a condition of sulphates <=0.585 and is also divided into one leaf node and the other internal node with a condition of residual sugar <=4.1. This internal node is ultimately divided into two leaf nodes with bad gini impurity of 0.513 and 0.56.\n",
    "On the other hand, the node with sulphates <=0.685 is further divided into two leaf nodes and is complete. The two leaf nodes has a moderate gini impurity of 0.55 and 0.585. Both of them is not homogeneous. The bias is high, that means the prediction will take a lot of assumption.And as the variance is low, any sample addition is not going to make a significant difference in the predicted result.\n",
    "\n",
    "#### 3) Maximum leaf nodes\n",
    "###### Part 1 Analysis: Max_depth = 5,max_leaf_nodes = 2,min_samples_leaf=10\n",
    "\n",
    "<img src='DecisionTree/Max_leaf_nodes_1.png' style='height:400px'/>\n",
    "\n",
    "This image provides a detailed analysis of why the tree having a max_depth of 5 only lands up to a depth of 1.\n",
    "The parameters given in making the decisiontree are max_depth,max_leaf_nodes and min_samples_split. As soon as the first parent node with a condition of alcohol<=10.15 splits into two leaf nodes, the parameter max_leaf_nodes is exhausted. We have two leaf nodes.One with a gini impurity of 0.517 and the other with a gini impurity of 0.656. Both of them are not homogeneous and have a high bias and low variance. This tree is therefore, not capable of producing a good prediction as the nodes are not in the purest form.\n",
    "\n",
    "###### Part 2 Analysis: Max_depth = 5,max_leaf_nodes = 3,min_samples_leaf=10\n",
    "\n",
    "<img src='DecisionTree/Max_leaf_nodes_2.png' style='height:400px'/>\n",
    "\n",
    "This image provides a detailed analysis of why the tree having a max_depth of 5 only lands up to a depth of 2.\n",
    "The parameters given in making the decisiontree are max_depth,max_leaf_nodes and min_samples_split. The first parent node is divided on the condition alcohol <=10.15.One of them with a condition of alcohol >10.15 is a leaf node, with a bad gini value of 0.656 which is not all homogeneous.The other internal node that is formed with the condition alcohol<= 10.15 is divided into two leaf nodes.The condition for splitting is total sulfur dioxide. One of them has total sulfur dioxide <=98.5 and the other has total sulfur dioxide >98.5. Both of them are leaf nodes. One of them is not though in its purest forms. Gini value is high amd close to 0.6. While the other one is more closer to its purest form . The gini value is 0.106 which is more nearer to 0.0. The bias for this node is relatively much lower and has a high variance. This is definitely good.So the total leaf nodes is already 3 but the max_depth is not 5 . Now , if any of the tree node gets divided into to leaf nodes , the max_leaf_nodes would be higher than 3 and that violates the parameter max_leaf_nodes. Henceforth, the max_depth is retained at 2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Neural Networks\n",
    "Neural networks seemed to be a good bet after decision trees. We have used sklearn's Multi Layer Perceptron library for our analysis. We have followed a train-test split of 75-25 and also scaled the data based on mean and standard deviation before feeding it to the training. We have trained 3 different neural networks where we tried a bunch of different parameters while training and selected the best ones based on the performance. During this exercise we tried various hidden layers with different number of neurons, 'logistic' & 'relu' activation functions and 'adam' & 'sgd' as our solvers. We have obeserved that 'relu' activation function with 'adam' solver gives best results if all the other parameters kept constant. We have also tried different values of maximum iterations while training the models ranging from 500 to 10000.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "The following is the frequency plot of each 'quality' category in the data:\n",
    "\n",
    "![frequency](EDA/categories_frequency.png)\n",
    "\n",
    "From the plot it can be seen that the data is highly unbalanced across the different categories and also we don't have very poor or very high quality wine in our dataset. To make it more balanced we divided the data into two categories namely 'good' and 'bad' based on the criteria expalined in the Analysis section. After breaking it down we got a quite balanced data with 744 instances in Bad quality(class=0) and 855 instances in Good quality(class=1) for wine.\n",
    "\n",
    "We also checked the correlations amongst different variables in the data and here are the results:\n",
    "\n",
    "![fcorr](EDA/correlation.png)\n",
    "\n",
    "Correlation heatmap shows that there ......(should I include new target variable)\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "#### Min_Samples_Leaf:\n",
    "<img src='DecisionTree/min_samples_result_1.png' style='height:200px'/>\n",
    "\n",
    "The parameters given are Max_depth is 3 and max_leaf_nodes is 10 while min_samples_leaf is 1000.Since the minimum number of samples in each leaf is given as 1000,if the parent and only node(depth=0) is divided into leaf nodes, then the samples size would have been lesser than 1000. This violates the parameter min_samples_leaf. Hence, although the max_depth is not 3, the tree is stopped, there is no breakage of internal nodes to leaf nodes and the gini parameter of the only node becomes 0.648. The gini impurity result implies that the only node is not at all homogeneous and the bias is high .Therefore , the results or predictions are going to be hugely assumed. This tree is not all in its purest form.\n",
    "\n",
    "<img src='DecisionTree/min_samples_result_2.png' style='height:300px'/>\n",
    "\n",
    "In contrast to the previous diagram, this decision tree has more leaf nodes and has lesser gini impurity values. The parameters given for making this tree are Max_depth is 3 and max_leaf_nodes is 10 while min_samples_leaf is 500. The primary reason for getting depth 1 leaf nodes is because of the minimum samples. This time , the minimum samples have been reduced from 1000 to 500. So, the tree can break into two leaf nodes and have a depth of 1. Though the gini impurity has decreased a bit yet it has a high value closer to 1.0 which is not at all pure. Hence,again the bias-variance trade off is not justified to the fullest. This tree has relatively higher bias and lower variance.The depth is 1 instead of 3.\n",
    "\n",
    "<img src='DecisionTree/min_samples_result_3.png' style='height:400px'/>\n",
    "\n",
    "Among all of the three diagrams, this is the best tree in terms of gini impurity and bias-variance importance. the depth of the tree is 3 which is fully shown in the diagram. The tree is broken into internal and leaf nodes on the basis of few features: alcohol, total sulfur dioxide,sulphates, volatile acidity. The leaf node that is formed from \"total sulfur dioxide <= 83.5 =False\" has the most significant gini impurity value ,0.244. This is closer to 0.0 . This leaf node has a low bias and a high variance. Any sample replacement will have a deep impact on the predicted class due to high variance. The gini values for class=6 is more than 0.5 . This is not pure and therefore, we can assume that any prediction of class_6 will be based on some assumption.But this time, the min_samples_leaf was 100 ,so when the depth of the tree becomes 3, the sampels in each node is still aobve 100. Thus making the depth ultimately attain a value of 3.\n",
    "\n",
    "#### Max_depth\n",
    "\n",
    "<img src='DecisionTree/max_depth_result_1.png' style='height:300px'/>\n",
    "\n",
    "The parameters are : Max_depth is 1 and max_leaf_nodes is 3 while min_samples_leaf is 10.The maximum depth parameter given in this diagram is 1 which is fully shown in the diagram. This picture shows the effectiveness of max_depth parameter. The algorithm produces a tree of 2 leaf nodes . This means that there is another room for om more leaf nodes. But as we see that max_depth is 1, the tree is already divided into two branches and therefore has depth of 1. Since the max_depth parameter is exhausted, no more leaf nodes is formed. Although, the main problem with the decision tree is its gini values. They are closer to 1.0 . Therefore, the tree is not at all in its purest form.\n",
    "\n",
    "<img src='DecisionTree/max_depth_result_2.png' style='height:400px'/>\n",
    "\n",
    "The parameters are : Max_depth is 2  while min_samples_leaf is 10. Like the previous diagram, this tree has also adhered to its depth parameter,that is 2. The attributes used for splitting the nodes are: alcohol, total sulfur dioxide. This time although the minimum samples leaf is way greater than 10, yet the tree does not have more depth than 2 due to max_depth parameter. The gini value is still closer to 0.6 for most of the nodes. There is one node that's relatively pure and has a gini value of 0.106. This is good since this node is almost pure. This means that this node has a lower bias and a higher variance. The sample size is 90 where sample 2 consists of 85.\n",
    "\n",
    "<img src='DecisionTree/max_depth_result_3.png' style='height:500px'/>\n",
    "\n",
    "This is most complex tree with parameters :Max_depth is 5 and max_leaf_nodes is 10 while min_samples_leaf is 10. On reaching the depth of 5, the leaf nodes aare exactly 10 and the min_samples_leaf is greater than 10. So,all the points are covered and all the parameters is taken care of. If the max_leaf_nodes would have been 20, then the parameter max_leaf_nodes would not have reached 20. This is because on a depth of 5, max leaf nodes is 10. So in order to produce more leaf nodes , depth has to increase . But the depth parameter is 5 which is already exhausted. Thus, the tree would have been exactly like this one even though the max_leaf_nodes was less than 20. There is no significant improvement in the gini value . All of them are nearer to 0.6. This leads to the fact that these tree nodes are not not pure and prediction will be based on assumption which is not reliable.\n",
    "\n",
    "\n",
    "#### Maximum leaf nodes\n",
    "\n",
    "<img src='DecisionTree/max_leaf_nodes_result_1.png' style='height:400px'/>\n",
    "\n",
    "The parameters for this diaagram are:Max_depth is 5 and max_leaf_nodes is 2 while min_samples_leaf is 10. Here we see that although the max_depth is 5 and min_samples_leaf is 10, the tree has only 1 depth. This is because of the parameter: max_leaf_nodes. After getting a depth of 1, we see that the tree has already reached 2 leaf nodes. Therefore, if the tree had expanded to depth 2, then the total number of leaf_nodes would have exceeded 2. This would have violated the parameter max_leaf_nodes which says 2. At deoth 1 , the 2 leaf_nodes is attained. Both the gini values are higher. So, we can safely assume that this tree can have more nodes and is not in its pure or homogeneous form. Moreover, the tree has higher bias which makes it vulnerable for assumption of unseen values.\n",
    "\n",
    "<img src='DecisionTree/max_leaf_nodes_result_2.png' style='height:500px'/>\n",
    "\n",
    "For this diagram, the parameters that have been used are: Max_depth is 5 and max_leaf_nodes is 3 while min_samples_leaf is 10. Here we again see that the tree abided by the parameter: max_leaf_nodes which is 3. Although the depth is in its basic stage : 2, the tree is not getting more depth. This is because, if the tree would have got depth:4, then the leaf_nodes would have surpassed 3 . This doesn't follow the parameter:max_leaf_nodes = 3 which is already attained. However, we find out the one leaf node has come closer to its purest form and has a gini value of 0.106. This means that the class_value is not based on assumption and has a low bias tradeoff error.\n",
    "\n",
    "<img src='DecisionTree/max_leaf_nodes_result_3.png' style='height:600px'/>\n",
    "\n",
    "At last in this diagram we see that the parameters:Max_depth is 5 and max_leaf_nodes is 10 while min_samples_leaf is 10 is fully utilized. The depth is 5 , max leaf nodes  are 10 and samples are way above 10. Unfortunately, the gini value doesn't get better. This tree although is deep and has a lot more nodes, the bias is not all good. This is because the gini values are still on a a higher range and hencforth the tree is not good for prediction .\n",
    "\n",
    "#### Finally the prediction results:\n",
    "We first found out the f1 and the accuracy score with the target variable as 3,4,5,6,7,8. \n",
    "\n",
    "<img src='DecisionTree/prediction_results_before_prunning.png' style='height:300px'/>\n",
    "\n",
    "Shockingly, we had a disastrous f1 score let alone accuracy. Henceforth , we decided to add a column which says that any quality under 5 would be considered as 'bad' or 0 . While anything having a quality of 6 or above will be 'good' or 1. Then we found out the beow statistics.\n",
    "\n",
    "<img src='DecisionTree/prediction_results.png' style='height:200px'/>\n",
    "\n",
    "The tree shows far better accuracy and f1_score. The f1_score has jumped 30% after the modification\n",
    "\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "We have tried various neural network architectures and trained the models by changing the number of hidden layers, neurons in hidden layers, different activation functions and solvers and finalized the following three architectures that performed well:\n",
    "\n",
    "##### Neural Network 1\n",
    "Architecture: 11 x 2 x 2 x 2 (2 hidden layers with 2 neurons in each layer)<br>\n",
    "Solver: Adam<br>\n",
    "Activation function: ReLU<br>\n",
    "Maximum iterations: 500\n",
    "\n",
    "![nn1](NeuralNetworks/NN1.png)\n",
    "\n",
    "*Results:*\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.67      0.73      0.70       178\n",
    "        1.0       0.77      0.71      0.74       222\n",
    "    avg / total   0.72      0.72      0.72       400\n",
    "\n",
    "\n",
    "##### Neural Network 2\n",
    "Architecture: 11 x 15 x 15 x 15 x 2 (3 hidden layers with 15 neurons in each layer)<br>\n",
    "Solver: Adam<br>\n",
    "Activation function: ReLU<br>\n",
    "Maximum iterations: 10000\n",
    "\n",
    "![nn2](NeuralNetworks/NN2.png)\n",
    "\n",
    "*Results:*\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.75      0.75      0.75       178\n",
    "        1.0       0.80      0.80      0.80       222\n",
    "    avg / total   0.78      0.78      0.78       400\n",
    "\n",
    "\n",
    "##### Neural Network 3\n",
    "Architecture: 11 x 30 x 30 x 15 x 2 (3 hidden layers with 30, 30 and 15 neurons in each layer respectively)<br>\n",
    "Solver: Adam<br>\n",
    "Activation function: ReLU<br>\n",
    "Maximum iterations: 10000\n",
    "\n",
    "![nn3](NeuralNetworks/NN3.png)\n",
    "\n",
    "*Results:*\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.78      0.79      0.79       178\n",
    "        1.0       0.83      0.82      0.83       222\n",
    "    avg / total   0.81      0.81      0.81       400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments relating to code snippet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code snippet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments relating to code snippet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
